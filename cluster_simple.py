"""
Production-–≤–∞—Ä–∏–∞–Ω—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ª–∏—Ü –Ω–∞ –±–∞–∑–µ ArcFace + HDBSCAN.
- –î–µ—Ç–µ–∫—Ü–∏—è –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: InsightFace (ArcFace), app.FaceAnalysis
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–Ω–∞—è HDBSCAN –ø–æ–≤–µ—Ä—Ö L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- –°–æ–≤–º–µ—Å—Ç–∏–º –ø–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É —Å —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π: build_plan_pro, distribute_to_folders, process_group_folder
- –£—Å—Ç–æ–π—á–∏–≤ –∫ Unicode-–ø—É—Ç—è–º, –º–Ω–æ–≥–æ-–ª–∏—Ü–∞–º –Ω–∞ —Ñ–æ—Ç–æ, –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—é –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
    pip install insightface onnxruntime opencv-python pillow scikit-learn numpy hdbscan

–ê–≤—Ç–æ—Ä: prod-ready —Å–∫–µ–ª–µ—Ç. –ü–æ–¥–∫–ª—é—á–∞–π—Ç–µ –≤ —Å–≤–æ—ë –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é.
"""
from __future__ import annotations
import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, Dict, Iterable, List, Optional, Tuple

import numpy as np
from PIL import Image
from collections import defaultdict

try:
    import cv2
except ImportError:
    cv2 = None

try:
    import hdbscan  # type: ignore
except Exception as e:  # pragma: no cover
    hdbscan = None

try:
    from insightface.app import FaceAnalysis
except Exception as e:  # pragma: no cover
    FaceAnalysis = None

IMG_EXTS = {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"}
ProgressCB = Optional[Callable[[str, int], None]]

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ)
QUALITY_THRESHOLD = 0.75  # –ë–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞
MIN_FACE_SIZE = 64        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ª–∏—Ü–∞ –≤ –ø–∏–∫—Å–µ–ª—è—Ö
MAX_FACE_SIZE = 512       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ª–∏—Ü–∞ –≤ –ø–∏–∫—Å–µ–ª—è—Ö
MAX_FACE_ANGLE = 30       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É–≥–æ–ª –ø–æ–≤–æ—Ä–æ—Ç–∞ –ª–∏—Ü–∞ (–≥—Ä–∞–¥—É—Å—ã)

# ------------------------
# –£—Ç–∏–ª–∏—Ç—ã –≤–≤–æ–¥–∞/–≤—ã–≤–æ–¥–∞
# ------------------------

def is_image(path: Path) -> bool:
    return path.suffix.lower() in IMG_EXTS

def validate_face_quality_dual(face: dict, img: np.ndarray) -> tuple[float, dict]:
    """
    –î–≤–æ–π–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º.

    Returns:
        tuple: (final_score, validation_details)
            - final_score: float –æ—Ç 0.0 –¥–æ 1.0
            - validation_details: dict —Å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    details = {
        'primary_score': 0.0,
        'secondary_score': 0.0,
        'cross_validation_score': 0.0,
        'final_score': 0.0,
        'method': 'dual_validation',
        'quality_metrics': {}
    }

    # –ü–µ—Ä–≤–∏—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è (–æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)
    primary_score = validate_face_quality(face, img)
    details['primary_score'] = primary_score

    # –í—Ç–æ—Ä–∏—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)
    secondary_score = validate_face_quality_alternative(face, img)
    details['secondary_score'] = secondary_score

    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
    cross_validation_score = 1.0 - min(abs(primary_score - secondary_score), 0.5)
    details['cross_validation_score'] = cross_validation_score

    # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä —Å —É—á–µ—Ç–æ–º –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω—ã - –ø–æ–≤—ã—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    # –ï—Å–ª–∏ –Ω–µ —Å–æ–≥–ª–∞—Å–Ω—ã - –ø–æ–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    agreement_factor = cross_validation_score  # 0.5-1.0
    average_score = (primary_score + secondary_score) / 2.0

    final_score = average_score * agreement_factor
    details['final_score'] = final_score

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    details['quality_metrics'] = _extract_quality_metrics(face, img)

    return final_score, details


def validate_face_quality_alternative(face: dict, img: np.ndarray) -> float:
    """
    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞ —Å –¥—Ä—É–≥–∏–º–∏ –≤–µ—Å–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.
    """
    scores = []
    weights = []

    # 1. –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π score –º–æ–¥–µ–ª–∏ (–±–æ–ª—å—à–∏–π –≤–µ—Å)
    if 'det_score' in face:
        det_score = float(face['det_score'])
        scores.append(min(det_score * 2.5, 1.0))  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        weights.append(0.5)  # –ë–æ–ª—å—à–æ–π –≤–µ—Å

    # 2. –†–∞–∑–º–µ—Ä –ª–∏—Ü–∞ (–º–µ–Ω—å—à–∏–π –≤–µ—Å)
    bbox = face.get('bbox', [])
    if len(bbox) == 4:
        x1, y1, x2, y2 = bbox
        face_width = abs(x2 - x1)
        face_height = abs(y2 - y1)
        min_size = min(face_width, face_height)

        if min_size < MIN_FACE_SIZE:
            return 0.0

        # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–∞–∑–º–µ—Ä—É
        if min_size < 100:
            size_score = min_size / 100.0
        elif min_size > 300:
            size_score = max(0.3, 1.0 - (min_size - 300) / 200.0)
        else:
            size_score = 1.0

        scores.append(size_score)
        weights.append(0.2)

    # 3. –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏)
    if len(bbox) == 4:
        blur_score = calculate_blur_score(img, bbox)
        scores.append(blur_score)
        weights.append(0.4)  # –ë–æ–ª—å—à–æ–π –≤–µ—Å –¥–ª—è —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏

    # 4. –û—Å–≤–µ—â–µ–Ω–Ω–æ—Å—Ç—å (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤–µ—Å)
    if len(bbox) == 4:
        brightness_score = calculate_brightness_score(img, bbox)
        scores.append(brightness_score)
        weights.append(0.3)

    # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
    if not scores:
        return 0.5

    if weights and len(weights) == len(scores):
        final_score = sum(s * w for s, w in zip(scores, weights)) / sum(weights)
    else:
        final_score = sum(scores) / len(scores)

    return min(final_score, 1.0)


def _rescue_low_quality_faces(rejected_faces: List[Dict], img: np.ndarray, img_path: Path) -> List[Dict]:
    """
    –£—Å—Ç–∞—Ä–µ–≤—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è rescue - –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ _advanced_rescue_low_quality_faces –¥–ª—è –Ω–æ–≤—ã—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π.
    """
    return _advanced_rescue_low_quality_faces(rejected_faces, img, img_path, None)


def _advanced_rescue_low_quality_faces(rejected_faces: List[Dict], img: np.ndarray, img_path: Path, embedder) -> List[Dict]:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å–ø–∞—Å—Ç–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ª–∏—Ü–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç AdvancedFaceRescue –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ rescue.
    """
    try:
        from app.services.face_detection.advanced_rescue import AdvancedFaceRescue, RescueStrategy

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è rescue —Å–∏—Å—Ç–µ–º—ã
        rescue_system = AdvancedFaceRescue(
            strategy=RescueStrategy.BALANCED,
            adaptive_learning=True
        )

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = _prepare_rescue_context(img, img_path, embedder)

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ rescue
        rescue_result = rescue_system.rescue_faces(rejected_faces, img, str(img_path), context)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if rescue_result.rescued_faces:
            print(f"üîÑ Advanced rescue: {len(rescue_result.rescued_faces)} faces rescued from {img_path}")
            for rec in rescue_result.recommendations[:2]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                print(f"   üí° {rec}")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º rescued –ª–∏—Ü–∞
        return rescue_result.rescued_faces

    except ImportError:
        # Fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ –µ—Å–ª–∏ AdvancedFaceRescue –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        print("‚ö†Ô∏è AdvancedFaceRescue not available, using legacy rescue")
        return _legacy_rescue_low_quality_faces(rejected_faces, img, img_path)


def _legacy_rescue_low_quality_faces(rejected_faces: List[Dict], img: np.ndarray, img_path: Path) -> List[Dict]:
    """
    Legacy –≤–µ—Ä—Å–∏—è rescue –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    """
    rescued_faces = []
    rescue_threshold = QUALITY_THRESHOLD * 0.6  # 60% –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞

    for face in rejected_faces:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if 'validation_details' in face:
            details = face['validation_details']

            # Rescue –∫—Ä–∏—Ç–µ—Ä–∏–∏:
            # 1. –•–æ—Ç—è –±—ã –æ–¥–∏–Ω —Å–∫–æ—Ä –≤—ã—à–µ rescue_threshold
            # 2. –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ > 0.7
            primary_ok = details['primary_score'] >= rescue_threshold
            secondary_ok = details['secondary_score'] >= rescue_threshold
            agreement_ok = details['cross_validation_score'] > 0.7

            if (primary_ok or secondary_ok) and agreement_ok:
                # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ rescued
                face['rescued'] = True
                face['rescue_reason'] = f"primary:{details['primary_score']:.2f}, secondary:{details['secondary_score']:.2f}"
                rescued_faces.append(face)
                print(f"üîÑ Rescued –ª–∏—Ü–æ –Ω–∞ {img_path} (score: {details['final_score']:.2f})")

        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –ª–∏—Ü - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫–æ—Ä
            quality_score = validate_face_quality(face, img)
            if quality_score >= rescue_threshold:
                face['rescued'] = True
                face['rescue_reason'] = f"basic_rescue:{quality_score:.2f}"
                rescued_faces.append(face)
                print(f"üîÑ Rescued –ª–∏—Ü–æ –Ω–∞ {img_path} (basic score: {quality_score:.2f})")

    return rescued_faces


def _prepare_rescue_context(img: np.ndarray, img_path: Path, embedder) -> Dict[str, Any]:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è rescue –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    context = {
        'image_height': img.shape[0] if len(img.shape) >= 2 else 0,
        'image_width': img.shape[1] if len(img.shape) >= 2 else 0,
        'image_faces_count': 0,  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
        'cluster_size': 1,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        'cluster_quality': 0.5,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        'temporal_context': 'single_image'
    }

    # –ï—Å–ª–∏ –µ—Å—Ç—å embedder, –ø–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    if embedder and hasattr(embedder, 'extract'):
        try:
            # –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Ü
            test_faces = embedder.extract(img)
            context['image_faces_count'] = len(test_faces)

            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if test_faces:
                avg_quality = sum(f.get('quality', 0.5) for f in test_faces) / len(test_faces)
                context['image_quality'] = avg_quality
        except Exception:
            pass

    return context


def _extract_quality_metrics(face: dict, img: np.ndarray) -> dict:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
    metrics = {}

    # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ DualFaceEmbedder
    if 'quality_details' in face:
        quality_details = face['quality_details']
        metrics.update({
            'detection_score': quality_details.get('detection_score', 0.0),
            'face_size_score': quality_details.get('face_size', 0.0),
            'blur_score': quality_details.get('blur_score', 0.0),
            'brightness_score': quality_details.get('brightness', 0.0),
            'overall_quality': quality_details.get('overall', 0.0)
        })

    # –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–µ—Ç–µ–∫—Ü–∏–∏
    metrics['source'] = face.get('source', 'unknown')
    metrics['model'] = face.get('model', 'unknown')
    metrics['cross_validated'] = face.get('cross_validated', False)

    return metrics


def validate_face_quality(face: dict, img: np.ndarray) -> float:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞.

    Returns:
        float: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç 0.0 –¥–æ 1.0
    """
    scores = []

    # 1. –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π score –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    if 'det_score' in face:
        det_score = float(face['det_score'])
        scores.append(min(det_score * 2.0, 1.0))  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 0-1

    # 2. –†–∞–∑–º–µ—Ä –ª–∏—Ü–∞
    bbox = face.get('bbox', [])
    if len(bbox) == 4:
        x1, y1, x2, y2 = bbox
        face_width = abs(x2 - x1)
        face_height = abs(y2 - y1)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        min_size = min(face_width, face_height)
        if min_size < MIN_FACE_SIZE:
            return 0.0  # –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ –ª–∏—Ü–æ
        if min_size > MAX_FACE_SIZE:
            return 0.0  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –ª–∏—Ü–æ

        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 128-256 –ø–∏–∫—Å–µ–ª–µ–π)
        size_score = 1.0
        if min_size < 128:
            size_score = min_size / 128.0
        elif min_size > 256:
            size_score = max(0.5, 1.0 - (min_size - 256) / 256.0)
        scores.append(size_score)

    # 3. –£–≥–æ–ª –ø–æ–≤–æ—Ä–æ—Ç–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    if 'pose' in face:
        pose = face['pose']
        if len(pose) >= 3:
            yaw, pitch, roll = pose[:3]
            max_angle = max(abs(yaw), abs(pitch), abs(roll))
            if max_angle > MAX_FACE_ANGLE:
                return 0.0  # –õ–∏—Ü–æ –ø–æ–≤–µ—Ä–Ω—É—Ç–æ —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ
            angle_score = 1.0 - (max_angle / 90.0)  # –ß–µ–º –º–µ–Ω—å—à–µ —É–≥–æ–ª, —Ç–µ–º –ª—É—á—à–µ
            scores.append(angle_score)

    # 4. –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ä–∞–∑–º—ã—Ç–æ—Å—Ç—å)
    if len(bbox) == 4:
        blur_score = calculate_blur_score(img, bbox)
        scores.append(blur_score)

    # 5. –û—Å–≤–µ—â–µ–Ω–Ω–æ—Å—Ç—å
    if len(bbox) == 4:
        brightness_score = calculate_brightness_score(img, bbox)
        scores.append(brightness_score)

    # –ò—Ç–æ–≥–æ–≤—ã–π score - —Å—Ä–µ–¥–Ω–µ–µ –≤—Å–µ—Ö –æ—Ü–µ–Ω–æ–∫
    if not scores:
        return 0.5  # –°—Ä–µ–¥–Ω–∏–π score, –µ—Å–ª–∏ –Ω–µ—Ç –æ—Ü–µ–Ω–æ–∫

    final_score = sum(scores) / len(scores)
    return min(final_score, 1.0)

def calculate_blur_score(img: np.ndarray, bbox: list) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏ –ª–∏—Ü–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.
    """
    try:
        x1, y1, x2, y2 = map(int, bbox)
        face_region = img[y1:y2, x1:x2]

        if face_region.size == 0:
            return 0.0

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ grayscale
        if len(face_region.shape) == 3:
            gray = cv2.cvtColor(face_region, cv2.COLOR_RGB2GRAY)
        else:
            gray = face_region

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Laplacian variance (–º–µ—Ä–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏)
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 0-1 (—Ö–æ—Ä–æ—à–∞—è —Ä–µ–∑–∫–æ—Å—Ç—å > 100)
        blur_score = min(laplacian_var / 100.0, 1.0)
        return blur_score

    except Exception:
        return 0.5  # –°—Ä–µ–¥–Ω–∏–π score –ø—Ä–∏ –æ—à–∏–±–∫–µ

def calculate_brightness_score(img: np.ndarray, bbox: list) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ –æ—Å–≤–µ—â–µ–Ω–Ω–æ—Å—Ç–∏ –ª–∏—Ü–∞.
    """
    try:
        x1, y1, x2, y2 = map(int, bbox)
        face_region = img[y1:y2, x1:x2]

        if face_region.size == 0:
            return 0.5

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π —è—Ä–∫–æ—Å—Ç–∏
        if len(face_region.shape) == 3:
            gray = cv2.cvtColor(face_region, cv2.COLOR_RGB2GRAY)
        else:
            gray = face_region

        brightness = np.mean(gray) / 255.0

        # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å 0.3-0.7
        if 0.3 <= brightness <= 0.7:
            return 1.0
        elif brightness < 0.3:
            return brightness / 0.3  # –¢–µ–º–Ω–æ
        else:
            return (1.0 - brightness) / 0.3  # –°–≤–µ—Ç–ª–æ

    except Exception:
        return 0.5  # –°—Ä–µ–¥–Ω–∏–π score –ø—Ä–∏ –æ—à–∏–±–∫–µ


def refine_clusters_two_stage(X: np.ndarray, owners: List[Path], labels: np.ndarray,
                             progress_callback: ProgressCB = None) -> Tuple[np.ndarray, List[Path], np.ndarray]:
    """
    –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

    Args:
        X: –ú–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        owners: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        labels: –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–∏—á–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (X, owners, labels) —Å —É—Ç–æ—á–Ω–µ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    if len(set(labels.tolist()) - {-1}) < 2:
        return X, owners, labels  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è

    # –≠—Ç–∞–ø 1: –ù–∞–π—Ç–∏ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    suspicious_cluster_ids = identify_suspicious_clusters_simple(X, labels)

    if not suspicious_cluster_ids:
        return X, owners, labels

    if progress_callback:
        progress_callback(f"üîÑ –ü–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {len(suspicious_cluster_ids)} —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 87)

    # –≠—Ç–∞–ø 2: –ü–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    refined_X = []
    refined_owners = []
    refined_labels = []

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    cluster_indices = {}
    for i, label in enumerate(labels):
        if label not in cluster_indices:
            cluster_indices[label] = []
        cluster_indices[label].append(i)

    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
    new_cluster_id = max(labels) + 1  # –ù–∞—á–∞—Ç—å —Å –Ω–æ–≤–æ–≥–æ ID

    for cluster_id in sorted(set(labels.tolist()) - {-1}):
        indices = cluster_indices[cluster_id]

        if cluster_id in suspicious_cluster_ids:
            # –ü–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä
            sub_X = X[indices]
            sub_labels = recluster_suspicious_cluster_simple(sub_X)

            # –î–æ–±–∞–≤–∏—Ç—å —Å—É–±–∫–ª–∞—Å—Ç–µ—Ä—ã —Å –Ω–æ–≤—ã–º–∏ ID
            for sub_label in set(sub_labels.tolist()) - {-1}:
                sub_indices = [indices[i] for i, lbl in enumerate(sub_labels) if lbl == sub_label]
                for idx in sub_indices:
                    refined_X.append(X[idx])
                    refined_owners.append(owners[idx])
                    refined_labels.append(new_cluster_id)
                new_cluster_id += 1
        else:
            # –û—Å—Ç–∞–≤–∏—Ç—å –Ω–∞–¥–µ–∂–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –∫–∞–∫ –µ—Å—Ç—å
            for idx in indices:
                refined_X.append(X[idx])
                refined_owners.append(owners[idx])
                refined_labels.append(cluster_id)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ –≤ numpy –º–∞—Å—Å–∏–≤—ã
    if refined_X:
        refined_X = np.vstack(refined_X)
        refined_labels = np.array(refined_labels)

        if progress_callback:
            progress_callback(f"‚úÖ –ü–æ—Å–ª–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è: {len(set(refined_labels.tolist()) - {-1})} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 90)

        return refined_X, refined_owners, refined_labels
    else:
        return X, owners, labels


def identify_suspicious_clusters_simple(X: np.ndarray, labels: np.ndarray) -> Set[int]:
    """
    –ù–∞–π—Ç–∏ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ –ø—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
    """
    suspicious = set()
    unique_labels = set(labels.tolist()) - {-1}

    for cluster_id in unique_labels:
        # –ù–∞–π—Ç–∏ –∏–Ω–¥–µ–∫—Å—ã –ª–∏—Ü –≤ —ç—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]

        if len(cluster_indices) < 3:
            suspicious.add(cluster_id)
            continue

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∞
        cluster_embeddings = X[cluster_indices]
        centroid = np.mean(cluster_embeddings, axis=0)
        distances = [np.linalg.norm(emb - centroid) for emb in cluster_embeddings]
        avg_distance = np.mean(distances)
        std_distance = np.std(distances)

        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if avg_distance > 0.4 or std_distance > 0.6:
            suspicious.add(cluster_id)

    return suspicious


def recluster_suspicious_cluster_simple(X: np.ndarray) -> np.ndarray:
    """
    –ü–µ—Ä–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    """
    if X.shape[0] < 3:
        # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤–µ—Ä–Ω—É—Ç—å –≤—Å–µ –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
        return np.zeros(X.shape[0], dtype=np.int32)

    # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    strict_min_cluster_size = max(2, X.shape[0] // 4)  # –ú–∏–Ω–∏–º—É–º 2 –∏–ª–∏ 1/4 –æ—Ç —Ä–∞–∑–º–µ—Ä–∞

    labels = cluster_embeddings_hdbscan(
        X,
        min_cluster_size=strict_min_cluster_size,
        min_samples=strict_min_cluster_size,
    )

    return labels


def post_process_clusters_simple(X: np.ndarray, owners: List[Path], labels: np.ndarray,
                                progress_callback: ProgressCB = None) -> Tuple[np.ndarray, List[Path], np.ndarray]:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.

    –ê–Ω–∞–ª–æ–≥ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤.
    """
    if len(set(labels.tolist()) - {-1}) < 2:
        return X, owners, labels

    if progress_callback:
        progress_callback("üîß –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 88)

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    cluster_indices = {}
    for i, label in enumerate(labels):
        if label not in cluster_indices:
            cluster_indices[label] = []
        cluster_indices[label].append(i)

    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    cluster_stats = analyze_clusters_quality_simple(X, labels, cluster_indices)

    # –£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–æ–≤—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    X, owners, labels = remove_noise_clusters_simple(X, owners, labels, cluster_indices)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    X, owners, labels = merge_similar_clusters_simple(X, owners, labels, cluster_indices)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    final_check = len(set(labels.tolist()) - {-1})
    if progress_callback:
        progress_callback(f"‚úÖ –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {final_check} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 95)

    return X, owners, labels


def analyze_clusters_quality_simple(X: np.ndarray, labels: np.ndarray,
                                   cluster_indices: Dict[int, List[int]]) -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –ø—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
    """
    stats = {}
    unique_labels = set(labels.tolist()) - {-1}

    for cluster_id in unique_labels:
        indices = cluster_indices[cluster_id]
        if len(indices) < 2:
            continue

        embeddings = X[indices]
        centroid = np.mean(embeddings, axis=0)
        distances = [np.linalg.norm(emb - centroid) for emb in embeddings]

        stats[cluster_id] = {
            'size': len(indices),
            'avg_distance': np.mean(distances),
            'std_distance': np.std(distances),
            'max_distance': np.max(distances)
        }

    return stats


def remove_noise_clusters_simple(X: np.ndarray, owners: List[Path], labels: np.ndarray,
                                cluster_indices: Dict[int, List[int]]) -> Tuple[np.ndarray, List[Path], np.ndarray]:
    """
    –£–¥–∞–ª–∏—Ç—å —à—É–º–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–∑ –ø—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
    """
    min_cluster_size = 2
    valid_indices = []

    for i, label in enumerate(labels):
        if label == -1:
            continue  # –®—É–º –≤—Å–µ–≥–¥–∞ —É–¥–∞–ª—è–µ–º

        cluster_size = len(cluster_indices.get(label, []))
        if cluster_size >= min_cluster_size:
            valid_indices.append(i)

    if not valid_indices:
        return X, owners, labels

    # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –º–∞—Å—Å–∏–≤—ã
    X_filtered = X[valid_indices]
    owners_filtered = [owners[i] for i in valid_indices]
    labels_filtered = labels[valid_indices]

    return X_filtered, owners_filtered, labels_filtered


def merge_similar_clusters_simple(X: np.ndarray, owners: List[Path], labels: np.ndarray,
                                 cluster_indices: Dict[int, List[int]]) -> Tuple[np.ndarray, List[Path], np.ndarray]:
    """
    –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –ø–æ—Ö–æ–∂–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ –ø—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
    """
    unique_labels = sorted(set(labels.tolist()) - {-1})
    if len(unique_labels) < 2:
        return X, owners, labels

    # –í—ã—á–∏—Å–ª–∏—Ç—å —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    centroids = {}
    for cluster_id in unique_labels:
        indices = cluster_indices[cluster_id]
        centroids[cluster_id] = np.mean(X[indices], axis=0)

    # –ù–∞–π—Ç–∏ –ø–∞—Ä—ã –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    merge_threshold = 0.75  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    to_merge = []

    for i, cid1 in enumerate(unique_labels):
        for cid2 in unique_labels[i+1:]:
            c1 = centroids[cid1]
            c2 = centroids[cid2]

            similarity = np.dot(c1, c2) / (np.linalg.norm(c1) * np.linalg.norm(c2))
            if similarity > merge_threshold:
                to_merge.append((cid1, cid2))

    # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    for cid1, cid2 in reversed(to_merge):
        # –ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ –º–µ—Ç–∫–∏ cid2 –Ω–∞ cid1
        labels = np.where(labels == cid2, cid1, labels)

    return X, owners, labels


def imread_safe(path: Path) -> Optional[np.ndarray]:
    """–ê–∫–∫—É—Ä–∞—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (BGR->RGB). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º cv2.imdecode –¥–ª—è –ª—É—á—à–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Unicode –ø—É—Ç–µ–π.
    """
    try:
        data = np.fromfile(str(path), dtype=np.uint8)
        img_bgr = cv2.imdecode(data, cv2.IMREAD_COLOR)
        if img_bgr is None:
            return None
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        return img_rgb
    except Exception:
        return None


# ------------------------
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ InsightFace
# ------------------------
@dataclass
class ArcFaceConfig:
    det_size: Tuple[int, int] = (640, 640)
    ctx_id: int = 0                   # GPU: –∏–Ω–¥–µ–∫—Å, CPU: -1
    allowed_blur: float = 0.8         # –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ (–ø—Ä–∏–º–µ—Ä–Ω—ã–π, –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º —è–≤–Ω—ã–π –º—É—Å–æ—Ä)


class ArcFaceEmbedder:
    def __init__(self, config: ArcFaceConfig = ArcFaceConfig(), model_name: str = "buffalo_l"):
        if FaceAnalysis is None:
            raise ImportError("insightface –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç insightface.")
        self.app = FaceAnalysis(name=model_name)
        # ctx_id=-1 ‚Üí CPU, –∏–Ω–∞—á–µ GPU. det_size –≤–ª–∏—è–µ—Ç –Ω–∞ recall/—Å–∫–æ—Ä–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        self.app.prepare(ctx_id=config.ctx_id, det_size=config.det_size)
        self.allowed_blur = config.allowed_blur

    def extract(self, img_rgb: np.ndarray) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ª–∏—Ü: [{embedding, quality, bbox}]. embedding —É–∂–µ L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω InsightFace."""
        faces = self.app.get(img_rgb)
        results: List[Dict] = []
        for f in faces:
            # f.normed_embedding ‚Äî L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (512,)
            emb = getattr(f, "normed_embedding", None)
            if emb is None:
                # –∑–∞–ø–∞—Å–Ω–æ–π –ø—É—Ç—å: normalise raw embedding
                raw = getattr(f, "embedding", None)
                if raw is None:
                    continue
                v = np.asarray(raw, dtype=np.float32)
                n = np.linalg.norm(v) + 1e-12
                emb = (v / n).astype(np.float32)
            else:
                emb = np.asarray(emb, dtype=np.float32)

            # —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º blur/pose/–¥–µ—Ç—Å–∫—É—é confidence –µ—Å–ª–∏ –µ—Å—Ç—å
            quality = float(getattr(f, "det_score", 0.99))
            if quality <= 0:  # —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞
                quality = 0.99

            bbox = tuple(int(x) for x in f.bbox.astype(int).tolist())
            results.append({
                "embedding": emb,
                "quality": quality,
                "bbox": bbox,
            })
        return results


def cluster_embeddings_hdbscan(
    embeddings: np.ndarray,
    min_cluster_size: int = 3,
    min_samples: Optional[int] = None,
) -> np.ndarray:
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º HDBSCAN."""
    if embeddings.size == 0:
        return np.array([], dtype=np.int32)
    if hdbscan is None:
        raise ImportError("hdbscan –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç hdbscan.")

    if embeddings.dtype != np.float32:
        embeddings = embeddings.astype(np.float32)

    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-12
    X = embeddings / norms

    clusterer = hdbscan.HDBSCAN(
        metric="euclidean",
        min_cluster_size=min_cluster_size,
        min_samples=min_samples or min_cluster_size,
        cluster_selection_epsilon=0.0,
        cluster_selection_method="eom",
    )
    labels = clusterer.fit_predict(X)

    uniq = sorted(x for x in set(labels.tolist()) if x != -1)
    remap = {old: i for i, old in enumerate(uniq)}
    out = labels.copy()
    for i, lb in enumerate(labels):
        out[i] = remap.get(int(lb), -1)
    return out


# ------------------------
# –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
# ------------------------

def build_plan_pro(
    input_dir: Path,
    progress_callback: ProgressCB = None,
    sim_threshold: float = 0.60,
    min_cluster_size: int = 2,
    ctx_id: int = 0,
    det_size: Tuple[int, int] = (640, 640),
    model_name: str = "buffalo_l",
    min_samples: Optional[int] = None,
    joint_mode: str = "copy",
) -> Dict:
    # sim_threshold —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ ‚Äî HDBSCAN –µ–≥–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç.
    """Production-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ª–∏—Ü —Å ArcFace + Faiss.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict:
      {
        "clusters": {"0": ["/abs/path/img1.jpg", ...], ...},
        "plan": [ {"path": str, "cluster": [int, ...], "faces": int}, ...],
        "unreadable": [str, ...],
        "no_faces": [str, ...]
      }
    """
    t0 = time.time()
    input_dir = Path(input_dir)
    if progress_callback:
        progress_callback(f"üöÄ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {input_dir}", 2)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–µ—Ä–∞
    # –î–ª—è buffalo_l –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–π det_size –µ—Å–ª–∏ –ø–∞–º—è—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞
    if model_name == "buffalo_l":
        # –ü—Ä–æ–±—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è buffalo_l
        try:
            emb = ArcFaceEmbedder(ArcFaceConfig(det_size=det_size, ctx_id=ctx_id), model_name=model_name)
        except Exception as e:
            print(f"Warning: buffalo_l failed with det_size {det_size}, trying smaller...")
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, –ø—Ä–æ–±—É–µ–º —Å –º–µ–Ω—å—à–∏–º —Ä–∞–∑–º–µ—Ä–æ–º
            smaller_det_size = (max(320, det_size[0] // 2), max(320, det_size[1] // 2))
            emb = ArcFaceEmbedder(ArcFaceConfig(det_size=smaller_det_size, ctx_id=ctx_id), model_name=model_name)
            print(f"Using buffalo_l with reduced det_size: {smaller_det_size}")
    else:
        emb = ArcFaceEmbedder(ArcFaceConfig(det_size=det_size, ctx_id=ctx_id), model_name=model_name)

    # –°–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    all_images = [p for p in input_dir.rglob("*") if p.is_file() and is_image(p)]
    if progress_callback:
        progress_callback(f"üìÇ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}", 5)

    owners: List[Path] = []
    all_embeddings: List[np.ndarray] = []
    img_face_count: Dict[Path, int] = {}
    unreadable: List[Path] = []
    no_faces: List[Path] = []

    total = len(all_images)
    for i, img_path in enumerate(all_images):
        if progress_callback and (i % 10 == 0):
            percent = 5 + int((i + 1) / max(1, total) * 60)
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑ {i+1}/{total}", percent)

        img = imread_safe(img_path)
        if img is None:
            unreadable.append(img_path)
            continue

        faces = emb.extract(img)
        if not faces:
            no_faces.append(img_path)
            continue

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü
        validated_faces = []
        rejected_faces = []

        for face in faces:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if 'quality_details' in face and 'cross_validated' in face:
                # –î–∞–Ω–Ω—ã–µ –æ—Ç DualFaceEmbedder - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–≤–æ–π–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
                quality_score, validation_details = validate_face_quality_dual(face, img)
                face['validation_details'] = validation_details  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            else:
                # –û–±—ã—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
                quality_score = validate_face_quality(face, img)

            if quality_score >= QUALITY_THRESHOLD:
                validated_faces.append(face)
            else:
                rejected_faces.append(face)
                print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –ª–∏—Ü–æ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ (score={quality_score:.2f}) –Ω–∞ {img_path}")
                if 'validation_details' in face:
                    details = face['validation_details']
                    print(f"   ‚îî‚îÄ –ü–µ—Ä–≤–∏—á–Ω—ã–π: {details['primary_score']:.2f}, "
                          f"–í—Ç–æ—Ä–∏—á–Ω—ã–π: {details['secondary_score']:.2f}, "
                          f"–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {details['cross_validation_score']:.2f}")

        # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ª–∏—Ü, –Ω–æ –µ—Å—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã–µ - –ø–æ–ø—Ä–æ–±—É–µ–º rescue –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ
        if not validated_faces and rejected_faces:
            rescued_faces = _advanced_rescue_low_quality_faces(rejected_faces, img, img_path, emb)
            validated_faces.extend(rescued_faces)

        if not validated_faces:
            no_faces.append(img_path)
            continue

        img_face_count[img_path] = len(validated_faces)
        for face in validated_faces:
            all_embeddings.append(face["embedding"])  # —É–∂–µ L2-–Ω–æ—Ä–º
            owners.append(img_path)

    if not all_embeddings:
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }

    X = np.vstack(all_embeddings).astype(np.float32)

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ HDBSCAN
    if progress_callback:
        progress_callback("üîó –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è HDBSCAN", 70)
    labels = cluster_embeddings_hdbscan(
        X,
        min_cluster_size=max(2, min_cluster_size),
        min_samples=min_samples,
    )

    if progress_callback:
        progress_callback(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(set(labels.tolist()) - {-1})}", 85)

    # –î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    X, owners, labels = refine_clusters_two_stage(X, owners, labels, progress_callback)

    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    X, owners, labels = post_process_clusters_simple(X, owners, labels, progress_callback)

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞–ø–æ–≤
    cluster_map: Dict[int, set[Path]] = defaultdict(set)
    cluster_by_img: Dict[Path, set[int]] = defaultdict(set)

    for lb, path in zip(labels, owners):
        if lb == -1:
            # –æ–¥–∏–Ω–æ—á–∫–∏: –º–æ–∂–Ω–æ –ø–æ–º–µ—Å—Ç–∏—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É "-1" –ª–∏–±–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏–∑ –ø–ª–∞–Ω–∞
            continue
        cluster_map[int(lb)].add(path)
        cluster_by_img[path].add(int(lb))

    # –ü–ª–∞–Ω –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–π/–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
    plan: List[Dict] = []
    for path in all_images:
        cl = cluster_by_img.get(path)
        if not cl:
            continue
        plan.append({
            "path": str(path),
            "cluster": sorted(list(cl)),
            "faces": img_face_count.get(path, 0),
        })

    if progress_callback:
        dt = time.time() - t0
        progress_callback(f"‚è±Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {dt:.1f}—Å", 95)

    return {
        "clusters": {str(k): [str(p) for p in sorted(v)] for k, v in cluster_map.items()},
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }


# ------------------------
# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–∞–ø–∫–∞–º (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π)
# ------------------------

def distribute_to_folders(plan: dict, base_dir: Path, cluster_start: int = 1, progress_callback: ProgressCB = None, common_mode: bool = False, joint_mode: str = "copy", post_validate: bool = False) -> Tuple[int, int, int]:
    import shutil

    moved, copied = 0, 0
    moved_paths = set()

    used_clusters = sorted({c for item in plan.get("plan", []) for c in item["cluster"]})
    # –í —Ä–µ–∂–∏–º–µ –û–ë–©–ê–Ø –ø–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Ç–µ—Ä—ã –ª—é–¥–µ–π —Å –æ–±—â–∏—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
    common_photo_clusters = set()
    if common_mode:
        # –ù–∞—Ö–æ–¥–∏–º –∫–ª–∞—Å—Ç–µ—Ä—ã –ª—é–¥–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –Ω–∞ –æ–±—â–∏—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è—Ö
        for item in plan.get("plan", []):
            src = Path(item["path"])
            is_common_photo = any(excluded_name in str(src.parent).lower() for excluded_name in EXCLUDED_COMMON_NAMES)
            if is_common_photo:
                common_photo_clusters.update(item["cluster"])

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å used_clusters —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –æ–±—â–∏—Ö —Ñ–æ—Ç–æ
        used_clusters = sorted(set(used_clusters) | common_photo_clusters)

    # –í —Ä–µ–∂–∏–º–µ common_mode —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –∏–Ω–∞—á–µ –ø–µ—Ä–µ–Ω—É–º–µ—Ä–æ–≤—ã–≤–∞–µ–º
    if common_mode:
        cluster_id_map = {old: old for old in used_clusters}  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–æ–º–µ—Ä–∞
    else:
        cluster_id_map = {old: cluster_start + idx for idx, old in enumerate(used_clusters)}

    plan_items = plan.get("plan", [])
    total_items = len(plan_items)
    if progress_callback:
        progress_callback(f"üîÑ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_items} —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º...", 0)

    cluster_file_counts: Dict[int, int] = {}
    for item in plan_items:
        src = Path(item["path"])
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –æ–±—â–∏–º (–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ "–æ–±—â–∏–µ")
        is_common_photo = any(excluded_name in str(src.parent).lower() for excluded_name in EXCLUDED_COMMON_NAMES)
        
        if not is_common_photo:  # –°—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ù–ï –æ–±—â–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
            clusters = [cluster_id_map[c] for c in item["cluster"]]
            for cid in clusters:
                cluster_file_counts[cid] = cluster_file_counts.get(cid, 0) + 1

    for i, item in enumerate(plan_items):
        if progress_callback:
            percent = int((i + 1) / max(total_items, 1) * 100)
            progress_callback(f"üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: {percent}% ({i+1}/{total_items})", percent)

        src = Path(item["path"])  # –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        if not src.exists():
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –æ–±—â–∏–º (–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ "–æ–±—â–∏–µ")
        is_common_photo = any(excluded_name in str(src.parent).lower() for excluded_name in EXCLUDED_COMMON_NAMES)
        
        if is_common_photo:
            # –û–±—â–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ù–ï –ø–µ—Ä–µ–º–µ—â–∞–µ–º - –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞ –º–µ—Å—Ç–µ
            print(f"üìå –û–±—â–∞—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –æ—Å—Ç–∞–≤–ª–µ–Ω–∞: {src.name}")
            continue

        if len(clusters) == 1:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞–ø–∫—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è: –±–µ—Ä–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –ø–∞–ø–∫—É —Ñ–∞–π–ª–∞
            parent_folder = src.parent
            dst = parent_folder / f"{clusters[0]}" / src.name
            dst.parent.mkdir(parents=True, exist_ok=True)
            if src.resolve() != dst.resolve():
                shutil.move(str(src), str(dst))
                moved += 1
                moved_paths.add(src.parent)
        else:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º—É–ª—å—Ç–∏-–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
            parent_folder = src.parent
            if joint_mode == "combine":
                # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø–∞–ø–∫—É
                combo_name = "+".join(str(c) for c in sorted(clusters))
                dst = parent_folder / combo_name / src.name
                dst.parent.mkdir(parents=True, exist_ok=True)
                if src.resolve() != dst.resolve():
                    shutil.move(str(src), str(dst))
                    moved += 1
                    moved_paths.add(src.parent)
            else:  # joint_mode == "copy" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                # –ö–æ–ø–∏—Ä—É–µ–º –≤ –∫–∞–∂–¥—É—é –ø–∞–ø–∫—É –∫–ª–∞—Å—Ç–µ—Ä–∞
                for cid in clusters:
                    dst = parent_folder / f"{cid}" / src.name
                    dst.parent.mkdir(parents=True, exist_ok=True)
                    if src.resolve() != dst.resolve():
                        shutil.copy2(str(src), str(dst))
                        copied += 1
                try:
                    src.unlink()
                except Exception:
                    pass

    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 95)

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –ø–∞–ø–∫–∏ –∏–∑ –ø–µ—Ä–µ–º–µ—â–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    parent_folders = set()
    for item in plan_items:
        src = Path(item["path"])
        if src.parent.exists():
            parent_folders.add(src.parent)

    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    for parent_folder in parent_folders:
        for cid in cluster_file_counts.keys():
            folder_path = parent_folder / str(cid)
            if folder_path.exists():
                # –°—á–∏—Ç–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
                real_count = 0
                for file_path in folder_path.iterdir():
                    if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
                        real_count += 1

                if real_count == 0:
                    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏
                    try:
                        folder_path.rmdir()
                        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–∞–ø–∫–∞: {folder_path}")
                    except Exception:
                        pass

    # –ß–∏—Å—Ç–∏–º –ø—É—Å—Ç—ã–µ –∫–∞—Ç–∞–ª–æ–≥–∏
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 100)
    for p in sorted(moved_paths, key=lambda x: len(str(x)), reverse=True):
        try:
            p.rmdir()
        except Exception:
            pass

    # –í —Ä–µ–∂–∏–º–µ –û–ë–©–ê–Ø —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏ –¥–ª—è –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ + 2 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ
    if common_mode:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –ø–∞–ø–∫–µ (–Ω–µ –≤ –ø–∞–ø–∫–µ "–æ–±—â–∏–µ")
        parent_dir = base_dir.parent
        print(f"üìÅ –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {parent_dir}")

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (—Ç–µ–ø–µ—Ä—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –Ω–æ–º–µ—Ä–∞–º–∏)
        for cluster_id in used_clusters:
            empty_folder = parent_dir / str(cluster_id)
            empty_folder.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø—É—Å—Ç–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞: {cluster_id} –≤ {parent_dir}")

        # –°–æ–∑–¥–∞–µ–º 2 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏
        max_cluster_id = max(used_clusters) if used_clusters else 0
        for i in range(1, 3):  # –°–æ–∑–¥–∞–µ–º 2 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞–ø–∫–∏
            extra_cluster_id = max_cluster_id + i
            extra_folder = parent_dir / str(extra_cluster_id)
            extra_folder.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—É—Å—Ç–∞—è –ø–∞–ø–∫–∞: {extra_cluster_id} –≤ {parent_dir}")

    # –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if post_validate:
        print(f"üîç [POST-VALIDATE] –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—é, post_validate={post_validate}")
        print(f"üîç [POST-VALIDATE] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(cluster_file_counts)}")
        if progress_callback:
            progress_callback("üîç –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 95)
        try:
            false_positives_moved = post_validate_clusters(base_dir, cluster_file_counts.keys(), progress_callback)
            print(f"‚úÖ [POST-VALIDATE] –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–µ–Ω–æ {false_positives_moved} —Ñ–æ—Ç–æ")
            if false_positives_moved > 0:
                print(f"‚ö†Ô∏è –í–æ–∑–≤—Ä–∞—â–µ–Ω–æ {false_positives_moved} —Ñ–æ—Ç–æ –∏–∑ false positive –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –ø–∞–ø–∫—É")
        except Exception as e:
            print(f"‚ùå [POST-VALIDATE] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –ø–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            import traceback
            traceback.print_exc()

    return moved, copied, cluster_start + len(used_clusters)


# ------------------------
# –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
# ------------------------

def post_validate_clusters(base_dir: Path, cluster_ids: Iterable[int], progress_callback: ProgressCB = None) -> int:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ false positives.
    –ï—Å–ª–∏ –≤ –ø–∞–ø–∫–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –æ–∫–∞–∑–∞–ª–∏—Å—å –ª–∏—Ü–∞ —Ä–∞–∑–Ω—ã—Ö –ª—é–¥–µ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–æ—Ç–æ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –ø–∞–ø–∫—É.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–º–µ—â–µ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ.
    """
    print(f"üîç [POST-VALIDATE] –§—É–Ω–∫—Ü–∏—è post_validate_clusters –∑–∞–ø—É—â–µ–Ω–∞")
    print(f"üîç [POST-VALIDATE] base_dir: {base_dir}")
    print(f"üîç [POST-VALIDATE] cluster_ids: {list(cluster_ids)}")

    total_moved = 0
    checked_clusters = 0

    for cluster_id in cluster_ids:
        print(f"üîç [POST-VALIDATE] –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä {cluster_id}")
        cluster_dir = base_dir / str(cluster_id)
        if not cluster_dir.exists():
            continue

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–æ—Ç–æ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–π –ø–∞–ø–∫–∏
        cluster_images = []
        for file_path in cluster_dir.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in IMG_EXTS:
                cluster_images.append(file_path)

        print(f"üîç [POST-VALIDATE] –í –∫–ª–∞—Å—Ç–µ—Ä–µ {cluster_id} –Ω–∞–π–¥–µ–Ω–æ {len(cluster_images)} —Ñ–æ—Ç–æ: {[p.name for p in cluster_images[:3]]}{'...' if len(cluster_images) > 3 else ''}")

        if len(cluster_images) < 2:
            # –ü–∞–ø–∫–∞ —Å 0-1 —Ñ–æ—Ç–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º
            print(f"üîç [POST-VALIDATE] –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} –ø—Ä–æ–ø—É—â–µ–Ω (—Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ñ–æ—Ç–æ)")
            continue

        checked_clusters += 1
        print(f"üîç [POST-VALIDATE] –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id} —Å {len(cluster_images)} —Ñ–æ—Ç–æ")
        if progress_callback and checked_clusters % 5 == 0:
            progress_callback(f"üîç –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {checked_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 95)

        # –ü–æ–≤—Ç–æ—Ä–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –ª–∏—Ü–∞ –∏–∑ —ç—Ç–æ–π –ø–∞–ø–∫–∏
        try:
            print(f"üîç [POST-VALIDATE] –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}")
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–ª–∞–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–æ–π –ø–∞–ø–∫–∏
            temp_plan = build_plan_pro(
                cluster_dir,
                progress_callback=None,  # –¢–∏—Ö–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                sim_threshold=0.6,
                min_cluster_size=2,
                ctx_id=0,
                det_size=(640, 640),
                joint_mode="copy"
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            clusters_in_folder = temp_plan.get("clusters", {})
            print(f"üîç [POST-VALIDATE] –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}: {len(clusters_in_folder)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            if len(clusters_in_folder) > 1:
                # False positive! –í –ø–∞–ø–∫–µ –æ–∫–∞–∑–∞–ª–∏—Å—å —Ä–∞–∑–Ω—ã–µ –ª—é–¥–∏
                print(f"‚ö†Ô∏è [POST-VALIDATE] False positive –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ {cluster_id}: –Ω–∞–π–¥–µ–Ω–æ {len(clusters_in_folder)} —Ä–∞–∑–Ω—ã—Ö –ª—é–¥–µ–π")
                print(f"‚ö†Ô∏è [POST-VALIDATE] –ö–ª–∞—Å—Ç–µ—Ä—ã –≤ –ø–∞–ø–∫–µ: {list(clusters_in_folder.keys())}")

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ —Ñ–æ—Ç–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –ø–∞–ø–∫—É (base_dir)
                for img_path in cluster_images:
                    dst = base_dir / img_path.name
                    counter = 1
                    while dst.exists():
                        stem = img_path.stem
                        suffix = img_path.suffix
                        dst = base_dir / f"{stem}_{counter}{suffix}"
                        counter += 1

                    shutil.move(str(img_path), str(dst))
                    total_moved += 1
                    print(f"‚Ü©Ô∏è [POST-VALIDATE] –§–æ—Ç–æ {img_path.name} –≤–æ–∑–≤—Ä–∞—â–µ–Ω–æ –≤ {base_dir}")

                # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—É—é –ø–∞–ø–∫—É –∫–ª–∞—Å—Ç–µ—Ä–∞
                try:
                    cluster_dir.rmdir()
                    print(f"üóëÔ∏è [POST-VALIDATE] –£–¥–∞–ª–µ–Ω–∞ false positive –ø–∞–ø–∫–∞: {cluster_dir}")
                except Exception as e:
                    print(f"‚ö†Ô∏è [POST-VALIDATE] –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –ø–∞–ø–∫—É {cluster_dir}: {e}")

        except Exception as e:
            print(f"‚ö†Ô∏è [POST-VALIDATE] –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}: {e}")
            import traceback
            traceback.print_exc()
            continue

    if progress_callback:
        progress_callback(f"‚úÖ –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {checked_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –≤–æ–∑–≤—Ä–∞—â–µ–Ω–æ {total_moved} —Ñ–æ—Ç–æ", 100)

    return total_moved


# ------------------------
# –ì—Ä—É–ø–ø–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ ¬´–æ–±—â–∏–µ¬ª –ø–∞–ø–∫–∏
# ------------------------

EXCLUDED_COMMON_NAMES = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]


def find_common_folders_recursive(group_dir: Path) -> List[Path]:
    common: List[Path] = []
    print(f"üîç –ò—â–µ–º –æ–±—â–∏–µ –ø–∞–ø–∫–∏ –≤: {group_dir}")
    for subdir in group_dir.rglob("*"):
        if subdir.is_dir():
            print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É: {subdir.name}")
            if any(ex in subdir.name.lower() for ex in EXCLUDED_COMMON_NAMES):
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –æ–±—â–∞—è –ø–∞–ø–∫–∞: {subdir}")
                common.append(subdir)
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –æ–±—â–∏—Ö –ø–∞–ø–æ–∫: {len(common)}")
    return common


def process_common_folder_at_level(common_dir: Path, progress_callback: ProgressCB = None,
                                   sim_threshold: float = 0.60, min_cluster_size: int = 2,
                                   ctx_id: int = 0, det_size: Tuple[int, int] = (640, 640)) -> Tuple[int, int]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ ¬´–æ–±—â–∏—Ö¬ª –ø–∞–ø–æ–∫: —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º –ª–∏—Ü–∞ –ø–æ –ø–æ–¥–ø–∞–ø–∫–∞–º –≤–Ω—É—Ç—Ä–∏ —Å–∞–º–æ–π ¬´–æ–±—â–µ–π¬ª.
    –ù–∞–ø—Ä–∏–º–µ—Ä: common/ ‚Üí common/1 (...), common/2 (...)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (moved, copied).
    """
    data = build_plan_pro(common_dir, progress_callback=progress_callback,
                          sim_threshold=sim_threshold, min_cluster_size=min_cluster_size,
                          ctx_id=ctx_id, det_size=det_size)
    moved, copied, _ = distribute_to_folders(data, common_dir, cluster_start=1, progress_callback=progress_callback, common_mode=True)
    return moved, copied


def process_group_folder(group_dir: Path, progress_callback: ProgressCB = None,
                         include_excluded: bool = False,
                         sim_threshold: float = 0.60, min_cluster_size: int = 2,
                         ctx_id: int = 0, det_size: Tuple[int, int] = (640, 640)) -> Tuple[int, int, int]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥—Ä—É–ø–ø—É –ø–æ–¥–ø–∞–ø–æ–∫: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç –∫–∞–∂–¥—É—é –ø–æ–¥–ø–∞–ø–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ.

    –ï—Å–ª–∏ include_excluded=False ‚Äî –ø–∞–ø–∫–∏ –∏–∑ EXCLUDED_COMMON_NAMES –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—Å—è.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (moved_total, copied_total, next_cluster_counter).
    """
    group_dir = Path(group_dir)

    if include_excluded:
        commons = find_common_folders_recursive(group_dir)
        for i, c in enumerate(commons):
            if progress_callback:
                progress_callback(f"üìã –û–±—â–∏–µ: {c.name} ({i+1}/{len(commons)})", 5 + int(i / max(1, len(commons)) * 20))
            process_common_folder_at_level(c, progress_callback=progress_callback,
                                           sim_threshold=sim_threshold, min_cluster_size=min_cluster_size,
                                           ctx_id=ctx_id, det_size=det_size)

    subdirs = [d for d in sorted(group_dir.iterdir()) if d.is_dir()]
    if not include_excluded:
        subdirs = [d for d in subdirs if all(ex not in d.name.lower() for ex in EXCLUDED_COMMON_NAMES)]

    total = len(subdirs)
    moved_all, copied_all = 0, 0
    next_cluster_id = 1  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å—á–µ—Ç—á–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –≤—Å–µ–π –≥—Ä—É–ø–ø—ã

    for i, sub in enumerate(subdirs):
        if progress_callback:
            progress_callback(f"üîç {sub.name}: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ({i+1}/{total})", 25 + int(i / max(1, total) * 60))
        data = build_plan_pro(
            input_dir=sub,
            progress_callback=progress_callback,
            sim_threshold=sim_threshold,
            min_cluster_size=min_cluster_size,
            ctx_id=ctx_id,
            det_size=det_size,
        )
        m, c, next_cluster_id = distribute_to_folders(data, sub, cluster_start=next_cluster_id, progress_callback=progress_callback)
        moved_all += m
        copied_all += c

    return moved_all, copied_all, next_cluster_id


# ------------------------
# CLI-–æ–±–≤—è–∑–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# ------------------------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ArcFace+Faiss face clustering")
    parser.add_argument("input", type=str, help="–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏–ª–∏ –≥—Ä—É–ø–ø–∞ –ø–∞–ø–æ–∫")
    parser.add_argument("--group", action="store_true", help="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∫ –≥—Ä—É–ø–ø—É –ø–æ–¥–ø–∞–ø–æ–∫")
    parser.add_argument("--include-common", action="store_true", help="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–∞–ø–∫–∏ '–æ–±—â–∏–µ' –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã")
    parser.add_argument("--sim", type=float, default=0.60, help="–ü–æ—Ä–æ–≥ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ [0..1]")
    parser.add_argument("--minsz", type=int, default=2, help="–ú–∏–Ω. —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞")
    parser.add_argument("--cpu", action="store_true", help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ CPU (ctx_id=-1)")
    parser.add_argument("--det", type=int, nargs=2, default=[640, 640], help="–†–∞–∑–º–µ—Ä –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ WxH")

    args = parser.parse_args()

    def cb(msg: str, p: int):
        print(f"[{p:3d}%] {msg}")

    if args.group:
        moved, copied, _ = process_group_folder(
            Path(args.input), progress_callback=cb,
            include_excluded=args.include_common,
            sim_threshold=args.sim, min_cluster_size=args.minsz,
            ctx_id=(-1 if args.cpu else 0), det_size=tuple(args.det),
        )
        print(f"DONE: moved={moved}, copied={copied}")
    else:
        data = build_plan_pro(
            Path(args.input), progress_callback=cb,
            sim_threshold=args.sim, min_cluster_size=args.minsz,
            ctx_id=(-1 if args.cpu else 0), det_size=tuple(args.det),
        )
        m, c, _ = distribute_to_folders(data, Path(args.input), cluster_start=1, progress_callback=cb)
        print(f"DONE: moved={m}, copied={c}")
